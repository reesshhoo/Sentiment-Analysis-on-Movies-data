{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing Libraries","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, LabelEncoder, FunctionTransformer, MinMaxScaler, StandardScaler, MaxAbsScaler, MultiLabelBinarizer\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegressionCV, LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier,BaggingClassifier\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom scipy.stats import loguniform, randint, uniform\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.svm import LinearSVC\nimport matplotlib.pyplot as plt\nimport sklearn.metrics as skm\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport re","metadata":{"execution":{"iopub.status.busy":"2023-08-11T15:42:53.602143Z","iopub.execute_input":"2023-08-11T15:42:53.603057Z","iopub.status.idle":"2023-08-11T15:42:56.095714Z","shell.execute_reply.started":"2023-08-11T15:42:53.603011Z","shell.execute_reply":"2023-08-11T15:42:56.093731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reading the Data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/sentiment-prediction-on-movie-reviews/train.csv')\ntest = pd.read_csv('/kaggle/input/sentiment-prediction-on-movie-reviews/test.csv')\nmovies = pd.read_csv('/kaggle/input/sentiment-prediction-on-movie-reviews/movies.csv')","metadata":{"execution":{"iopub.status.busy":"2023-08-11T15:42:56.098427Z","iopub.execute_input":"2023-08-11T15:42:56.098819Z","iopub.status.idle":"2023-08-11T15:42:58.713802Z","shell.execute_reply.started":"2023-08-11T15:42:56.098789Z","shell.execute_reply":"2023-08-11T15:42:58.711933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploring the Data","metadata":{}},{"cell_type":"markdown","source":"## Train data","metadata":{}},{"cell_type":"markdown","source":"#### Let's start by exploring the train dataset. We'll start by checking the features and it's type in teh train dataset.","metadata":{}},{"cell_type":"code","source":"print(\"The columns in the train dataset are\",train.columns.tolist())\nprint(\"The shape of the test dataset is\",train.shape)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T15:42:58.729121Z","iopub.execute_input":"2023-08-11T15:42:58.730565Z","iopub.status.idle":"2023-08-11T15:42:58.741921Z","shell.execute_reply.started":"2023-08-11T15:42:58.730497Z","shell.execute_reply":"2023-08-11T15:42:58.740031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2023-08-11T15:42:58.743555Z","iopub.execute_input":"2023-08-11T15:42:58.744157Z","iopub.status.idle":"2023-08-11T15:42:59.033978Z","shell.execute_reply.started":"2023-08-11T15:42:58.744123Z","shell.execute_reply":"2023-08-11T15:42:59.032447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### It is visible that there are no many numerical features in the train dataset. Therefore, we have to apply some type of encoding on the features we will use to train our model.","metadata":{}},{"cell_type":"markdown","source":"#### Now let's check for the missing values in the train data.","metadata":{}},{"cell_type":"code","source":"train.isna().mean()","metadata":{"execution":{"iopub.status.busy":"2023-08-11T15:42:59.035802Z","iopub.execute_input":"2023-08-11T15:42:59.036235Z","iopub.status.idle":"2023-08-11T15:42:59.268751Z","shell.execute_reply.started":"2023-08-11T15:42:59.036198Z","shell.execute_reply":"2023-08-11T15:42:59.267569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Here, we see that the train dataset has very few missing values.\n#### The only column with missing values here is 'reviewText' and even that is below the threshold of 0.05%. As a rule of thumb, it is an accepted practice to drop columns with missing values if they constitute less than 0.05% of the data.\n#### However, we will instead be impputing the null values in this column with empty string instead because we do not have the option of dropping the null values from the test data.","metadata":{}},{"cell_type":"markdown","source":"#### Now let's try to plot some graphs with the features.","metadata":{}},{"cell_type":"code","source":"sns.set(style=\"darkgrid\")\n\nsns.countplot(x='isFrequentReviewer', data=train)\nplt.xlabel('isFrequentReviewer')\nplt.ylabel('Count')\nplt.title('Count of Frequent Reviewers')","metadata":{"execution":{"iopub.status.busy":"2023-08-11T15:42:59.270987Z","iopub.execute_input":"2023-08-11T15:42:59.272427Z","iopub.status.idle":"2023-08-11T15:42:59.647473Z","shell.execute_reply.started":"2023-08-11T15:42:59.272378Z","shell.execute_reply":"2023-08-11T15:42:59.646445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### We can derive from this countplot that the number of reviewers who are not frequent outweigh the number of frequent reviewers by a good margin.","metadata":{}},{"cell_type":"code","source":"sns.set(style=\"darkgrid\")\n\nsns.countplot(x='sentiment', data=train)\nplt.xlabel('Sentiment')\nplt.ylabel('Count')\nplt.title('Sentiment on movies')","metadata":{"execution":{"iopub.status.busy":"2023-08-11T15:42:59.649202Z","iopub.execute_input":"2023-08-11T15:42:59.649877Z","iopub.status.idle":"2023-08-11T15:43:00.169646Z","shell.execute_reply.started":"2023-08-11T15:42:59.649839Z","shell.execute_reply":"2023-08-11T15:43:00.168504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### It is visible that there are more Positive reviews on the movies as opposed to the negative review. This plot resembles the plot that we visualized earlier using the 'isFrequentReviewer' column. It can be a possible hypothesis that the non-frequent reviewers are more likely to give a positive review to a movie as opposed to frequent reviewers. However, we are yet to properly explore and analyze the data so this is not something we should let cloud any future insights that may be drawn from the data.","metadata":{}},{"cell_type":"markdown","source":"#### Let's try to visualize the top 50 words in 'reviewText' column.","metadata":{}},{"cell_type":"code","source":"corpus = train['reviewText'].astype(str).fillna('')\nvectorizer = CountVectorizer()\nX = vectorizer.fit_transform(corpus)\ncounts = np.sum(X.toarray(), axis=0)\n\nwords = vectorizer.get_feature_names_out()\n\ncount_dict = {}\nfor i in range(len(counts)):\n    count_dict[words[i]] = counts[i]\n\ncount_dict = {k:v for k, v in sorted(count_dict.items(), key=lambda item: item[1], reverse=True)}\n","metadata":{"execution":{"iopub.status.busy":"2023-08-11T15:43:00.171053Z","iopub.execute_input":"2023-08-11T15:43:00.171431Z","iopub.status.idle":"2023-08-11T15:43:48.607759Z","shell.execute_reply.started":"2023-08-11T15:43:00.171398Z","shell.execute_reply":"2023-08-11T15:43:48.606585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,8))\nsns.barplot(x=list(count_dict.keys())[:50],y=list(count_dict.values())[:50])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-11T15:43:48.613507Z","iopub.execute_input":"2023-08-11T15:43:48.613997Z","iopub.status.idle":"2023-08-11T15:43:49.687643Z","shell.execute_reply.started":"2023-08-11T15:43:48.613914Z","shell.execute_reply":"2023-08-11T15:43:49.686154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### From the above plot, it is very evident that there are a lot of stop words like 'the', 'it','its','in','and' etc. We will try to remove as many of these stopwords as possible in the 'reviewText' column as they provide no meaningful information and we can reduce the number of unncessary features by doint this as well.","metadata":{}},{"cell_type":"markdown","source":"#### We shall now move on to explore the movies dataset.","metadata":{}},{"cell_type":"markdown","source":"## Movies ","metadata":{}},{"cell_type":"markdown","source":"#### First, Let's see what are the feature in the movies dataset and what type of features are they. ","metadata":{}},{"cell_type":"code","source":"print(\"The columns in the movies dataset are\",movies.columns.tolist())\nprint(\"The shape of the movies dataset is\",movies.shape)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T15:43:49.689384Z","iopub.execute_input":"2023-08-11T15:43:49.689749Z","iopub.status.idle":"2023-08-11T15:43:49.696693Z","shell.execute_reply.started":"2023-08-11T15:43:49.689719Z","shell.execute_reply":"2023-08-11T15:43:49.695015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"movies.info()","metadata":{"execution":{"iopub.status.busy":"2023-08-11T15:43:49.698670Z","iopub.execute_input":"2023-08-11T15:43:49.699055Z","iopub.status.idle":"2023-08-11T15:43:50.221749Z","shell.execute_reply.started":"2023-08-11T15:43:49.699023Z","shell.execute_reply":"2023-08-11T15:43:50.220297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"movies.isna().mean()","metadata":{"execution":{"iopub.status.busy":"2023-08-11T15:43:50.223703Z","iopub.execute_input":"2023-08-11T15:43:50.224147Z","iopub.status.idle":"2023-08-11T15:43:50.726222Z","shell.execute_reply.started":"2023-08-11T15:43:50.224103Z","shell.execute_reply":"2023-08-11T15:43:50.724656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### There are a lot of features in the movies dataset. However, we can see from the information being displayed above that there are also a lot of missing values in these columns. Some of them can be promptly dealt with using Imputation techniques but there are also columns such as 'rating', 'ratingContents' etc. where the no. of missing values exceed more than 90% of the dataset. Such columns shall not be considered as training features for the model as the data that is available might not be a representative sample of the overall population. This can lead to biased or skewed distributions and impact the performance of our model.","metadata":{}},{"cell_type":"code","source":"movies[movies.duplicated()]","metadata":{"execution":{"iopub.status.busy":"2023-08-11T15:43:50.727833Z","iopub.execute_input":"2023-08-11T15:43:50.728276Z","iopub.status.idle":"2023-08-11T15:43:51.145930Z","shell.execute_reply.started":"2023-08-11T15:43:50.728241Z","shell.execute_reply":"2023-08-11T15:43:51.144822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Taking a quick look at the data we can see there are a lot of missing values as well as many duplicates as well. \n#### We will drop the duplicate values before merging the relevant features with the train and test set.","metadata":{}},{"cell_type":"markdown","source":"#### Let's try to plot some visualizations for the featres in movies data as well","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(45,15))\nsns.set(style=\"darkgrid\")\n# palette = sns.color_palette(\"viridis\")\nsns.countplot(x='audienceScore', data=movies)\n\nplt.xlabel('Audience Score')\nplt.ylabel('Count')\nplt.title('Audience Score')","metadata":{"execution":{"iopub.status.busy":"2023-08-11T15:43:51.147491Z","iopub.execute_input":"2023-08-11T15:43:51.148507Z","iopub.status.idle":"2023-08-11T15:43:53.867815Z","shell.execute_reply.started":"2023-08-11T15:43:51.148466Z","shell.execute_reply":"2023-08-11T15:43:53.866102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.set(style=\"darkgrid\")\nsns.histplot(x='audienceScore', data=movies, bins=50, color='seagreen')\nplt.xlabel('Audience Score')\nplt.ylabel('Count')\nplt.title('Audience Score')","metadata":{"execution":{"iopub.status.busy":"2023-08-11T15:43:53.869824Z","iopub.execute_input":"2023-08-11T15:43:53.870238Z","iopub.status.idle":"2023-08-11T15:43:54.572756Z","shell.execute_reply.started":"2023-08-11T15:43:53.870200Z","shell.execute_reply":"2023-08-11T15:43:54.571384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Here, we can see the distribution for the Audience Scores. There are different scores with different counts. One thing to notice here is that there are quite a number fo movies with 0 score and also very few movies get the score between 90.0 - 99.0 but there are many movies which have 100.0 score.","metadata":{}},{"cell_type":"markdown","source":"#### Let's check the relationship between the two numerical columns of the movies dataset.","metadata":{}},{"cell_type":"code","source":"sns.jointplot(x = 'audienceScore', y = 'runtimeMinutes', data = movies, kind = 'hex', gridsize = 20)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T15:43:54.574295Z","iopub.execute_input":"2023-08-11T15:43:54.574638Z","iopub.status.idle":"2023-08-11T15:43:57.877158Z","shell.execute_reply.started":"2023-08-11T15:43:54.574608Z","shell.execute_reply":"2023-08-11T15:43:57.875766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### The jointplot shows that there isn't a strong relationship between the features 'audienceScore' and 'runtimeMinutes'.|","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,5))\nsns.set(style=\"darkgrid\")\n\nsns.violinplot(x='runtimeMinutes', data=movies)\nplt.xlabel('Runtime in Minutes')\nplt.ylabel('Count')\nplt.title('Runtime of movies')","metadata":{"execution":{"iopub.status.busy":"2023-08-11T15:43:57.878710Z","iopub.execute_input":"2023-08-11T15:43:57.879049Z","iopub.status.idle":"2023-08-11T15:43:58.596585Z","shell.execute_reply.started":"2023-08-11T15:43:57.879020Z","shell.execute_reply":"2023-08-11T15:43:58.595245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### The \"runtimeMinutes\" column has quite a lot of outliers as we can see on this violinplot. This makes it harder to see any patterns in the data. Dea;ling with outliers is an important step of data preprocessing as outlier detection is a crucial data preprocessing step to improve data quality, ensure model accuracy, and maintain statistical assumptions. Removing or handling outliers enhances data interpretation, model robustness, and performance. ","metadata":{}},{"cell_type":"markdown","source":"#### Since we have noticed the outlliers in this column, let's check for outliers in other numerical columns of the movies dataset as well.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(16,5))\nfig, axes = plt.subplots(nrows=2, ncols=1, figsize=(16, 10), sharey=True)\n\nsns.set(style=\"darkgrid\")\n\nsns.boxplot(x='runtimeMinutes', data=movies, ax=axes[0])\n\nsns.boxplot(x='audienceScore', data=movies, ax=axes[1])\n","metadata":{"execution":{"iopub.status.busy":"2023-08-11T15:43:58.598912Z","iopub.execute_input":"2023-08-11T15:43:58.599381Z","iopub.status.idle":"2023-08-11T15:43:59.297949Z","shell.execute_reply.started":"2023-08-11T15:43:58.599347Z","shell.execute_reply":"2023-08-11T15:43:59.296755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Features with many outliers are unsuitable for model training due to their influence on model performance, distorted relationships, and potential overfitting. Outliers violate assumptions, hinder interpretability, and challenge data normalization. Options include outlier removal, transformations, and using outlier-resistant models, while careful consideration and validation are essential.\n#### And the above plot makes it clear that 'runtimeMinutes' is not a suitable column for training our model. The joinplot that we made earlier can be an assurance that we're not missing out on any important input by dropping the 'runtimeMinutes' column.","metadata":{}},{"cell_type":"markdown","source":"#### Additionally, we will be dropping the columns 'rating', 'ratingContents', 'releaseDateTheaters', 'genre', 'originalLanguage', , 'boxOffice', 'distributor', 'soundType' because they have more than 50% missing values. \n#### The columns 'releaseDateStreaming','genre' and 'director' are also being dropped as they do not seem relevant to our purpose to predicting sentiment of ReviewText.","metadata":{}},{"cell_type":"markdown","source":"#### Let's check the test dataset now","metadata":{}},{"cell_type":"markdown","source":"## Test","metadata":{}},{"cell_type":"code","source":"print(\"The columns in the train dataset are\",test.columns.tolist())\nprint(\"The shape of the test dataset is\",test.shape)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T15:43:59.299027Z","iopub.execute_input":"2023-08-11T15:43:59.299356Z","iopub.status.idle":"2023-08-11T15:43:59.306499Z","shell.execute_reply.started":"2023-08-11T15:43:59.299329Z","shell.execute_reply":"2023-08-11T15:43:59.305128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.info()","metadata":{"execution":{"iopub.status.busy":"2023-08-11T15:43:59.308009Z","iopub.execute_input":"2023-08-11T15:43:59.308369Z","iopub.status.idle":"2023-08-11T15:43:59.378581Z","shell.execute_reply.started":"2023-08-11T15:43:59.308341Z","shell.execute_reply":"2023-08-11T15:43:59.376926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### We can see here that just like the train dataset, this dataset also has missing values only in the 'reviewText' column. We will deal with this in the same manner that we will be dealing with the missing values in the train dataset.\n#### It should also be mentioned that we will assume the \"isTopCritic\" column to be the same as the \"isFrequentReviewer\" in the train dataset.","metadata":{}},{"cell_type":"code","source":"test.rename(columns={'isTopCritic':'isFrequentReviewer'},inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T15:43:59.380998Z","iopub.execute_input":"2023-08-11T15:43:59.381496Z","iopub.status.idle":"2023-08-11T15:43:59.389636Z","shell.execute_reply.started":"2023-08-11T15:43:59.381451Z","shell.execute_reply":"2023-08-11T15:43:59.388075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocesing","metadata":{}},{"cell_type":"markdown","source":"### Now, we shall preprocess the data and get it ready in order to train the model on it. \n### We will start by selecting the features we need from the movies dataset and drop the duplicate values.\n### Then we will proceed to merge them with the train and test set. Following that, we will apply the necessary encoding and Imputation techniques to rid the data of any discrepancies. Lastly, we shall use some techniques to clean the text columns and get the final data prepared to train our models on.","metadata":{}},{"cell_type":"markdown","source":"### Selecting relevant features from Movies dataset","metadata":{}},{"cell_type":"code","source":"movies.drop(columns=['rating',\n 'ratingContents',\n 'releaseDateTheaters',\n 'boxOffice',\n 'distributor',\n 'soundType',\n 'releaseDateStreaming',\n 'director',\n 'runtimeMinutes',\n 'originalLanguage'], axis=1, inplace=True)\nmovies.drop_duplicates(subset=['movieid'], keep='last', inplace=True)\nmovies.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T15:43:59.391795Z","iopub.execute_input":"2023-08-11T15:43:59.392191Z","iopub.status.idle":"2023-08-11T15:43:59.489716Z","shell.execute_reply.started":"2023-08-11T15:43:59.392146Z","shell.execute_reply":"2023-08-11T15:43:59.488237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Merging the train and Test datasets with the movies data","metadata":{}},{"cell_type":"code","source":"merged_train = pd.merge(train, movies, on='movieid', how='left')\nmerged_test = pd.merge(test, movies, on='movieid', how='left')\nmerged_train.drop(['reviewerName','movieid'],axis=1,inplace=True)\nmerged_test.drop(['reviewerName','movieid'],axis=1,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T15:43:59.491548Z","iopub.execute_input":"2023-08-11T15:43:59.491897Z","iopub.status.idle":"2023-08-11T15:43:59.848435Z","shell.execute_reply.started":"2023-08-11T15:43:59.491868Z","shell.execute_reply":"2023-08-11T15:43:59.846858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Applying Imputations ","metadata":{}},{"cell_type":"markdown","source":"#### Replacing the null values in the data with empty strings.","metadata":{}},{"cell_type":"code","source":"merged_train['reviewText']= merged_train['reviewText'].astype(str).fillna('')\nmerged_test['reviewText'] = merged_test['reviewText'].astype(str).fillna('')","metadata":{"execution":{"iopub.status.busy":"2023-08-11T15:43:59.850069Z","iopub.execute_input":"2023-08-11T15:43:59.850457Z","iopub.status.idle":"2023-08-11T15:43:59.921359Z","shell.execute_reply.started":"2023-08-11T15:43:59.850426Z","shell.execute_reply":"2023-08-11T15:43:59.919998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_train.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-08-11T15:43:59.922933Z","iopub.execute_input":"2023-08-11T15:43:59.923369Z","iopub.status.idle":"2023-08-11T15:44:00.151276Z","shell.execute_reply.started":"2023-08-11T15:43:59.923332Z","shell.execute_reply":"2023-08-11T15:44:00.149905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imputer = SimpleImputer(strategy='mean')\nmerged_train[['audienceScore']] = imputer.fit_transform(merged_train[['audienceScore']])\nmerged_test[['audienceScore']] = imputer.fit_transform(merged_test[['audienceScore']])","metadata":{"execution":{"iopub.status.busy":"2023-08-11T15:44:00.152865Z","iopub.execute_input":"2023-08-11T15:44:00.153204Z","iopub.status.idle":"2023-08-11T15:44:00.200539Z","shell.execute_reply.started":"2023-08-11T15:44:00.153176Z","shell.execute_reply":"2023-08-11T15:44:00.199263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imputer = SimpleImputer(strategy='most_frequent')\nmerged_train[['genre']] = imputer.fit_transform(merged_train[['genre']])\nmerged_test[['genre']] = imputer.fit_transform(merged_test[['genre']])","metadata":{"execution":{"iopub.status.busy":"2023-08-11T15:44:00.211412Z","iopub.execute_input":"2023-08-11T15:44:00.211759Z","iopub.status.idle":"2023-08-11T15:44:00.294862Z","shell.execute_reply.started":"2023-08-11T15:44:00.211731Z","shell.execute_reply":"2023-08-11T15:44:00.293421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# getting unique Genres\nmerged_train['genre'] = merged_train['genre'].str.split(', ')\nmerged_test['genre'] = merged_test['genre'].str.split(', ')\nunique_genres_train = set()\nunique_genres_test = set()\nfor genres_list in merged_train['genre']:\n    unique_genres_train.update(genres_list)\nfor genres_list in merged_test['genre']:\n    unique_genres_test.update(genres_list)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T15:44:00.297097Z","iopub.execute_input":"2023-08-11T15:44:00.297484Z","iopub.status.idle":"2023-08-11T15:44:01.043768Z","shell.execute_reply.started":"2023-08-11T15:44:00.297454Z","shell.execute_reply":"2023-08-11T15:44:01.042303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Iterate through each unique genre\nfor i in unique_genres_train:\n    merged_train[i] = merged_train['genre'].apply(lambda x: 1 if i in x else 0)\nfor i in unique_genres_test:\n    merged_test[i] = merged_test['genre'].apply(lambda x: 1 if i in x else 0)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T15:44:01.045307Z","iopub.execute_input":"2023-08-11T15:44:01.045681Z","iopub.status.idle":"2023-08-11T15:44:07.099849Z","shell.execute_reply.started":"2023-08-11T15:44:01.045647Z","shell.execute_reply":"2023-08-11T15:44:07.098728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_train.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-08-11T15:44:07.101349Z","iopub.execute_input":"2023-08-11T15:44:07.101769Z","iopub.status.idle":"2023-08-11T15:44:07.341862Z","shell.execute_reply.started":"2023-08-11T15:44:07.101729Z","shell.execute_reply":"2023-08-11T15:44:07.340517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_test.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-08-11T15:44:07.343234Z","iopub.execute_input":"2023-08-11T15:44:07.343589Z","iopub.status.idle":"2023-08-11T15:44:07.421482Z","shell.execute_reply.started":"2023-08-11T15:44:07.343562Z","shell.execute_reply":"2023-08-11T15:44:07.420372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### With this, we have taken care of all the null values in our train and test set.","metadata":{}},{"cell_type":"code","source":"merged_test.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-11T15:44:07.423231Z","iopub.execute_input":"2023-08-11T15:44:07.424243Z","iopub.status.idle":"2023-08-11T15:44:07.451489Z","shell.execute_reply.started":"2023-08-11T15:44:07.424204Z","shell.execute_reply":"2023-08-11T15:44:07.450062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Processing the Text columns","metadata":{}},{"cell_type":"markdown","source":"#### Since we are prohibited from using nltk or similar language processing libraries, we will make a custom list of stop words and then remove them from our dataset.","metadata":{}},{"cell_type":"code","source":"stop_words = [\n    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves',\n    'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their',\n    'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was',\n    'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the',\n    'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against',\n    'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in',\n    'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why',\n    'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only',\n    'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now','so', 'too',\n    're', 'over', 'she', 'me', 'are', 'while', 'has', \"she's\", 'by', 'will', 'about', 'but', 'd', 'into', 'who', 's',\n    'no', 'or', 'is', 'above', \"mustn't\", 'before', 'i', 'he', \"mightn't\", 'aren', 'now', 'my', 'why', \"won't\", 'a',\n    'same', 'hers', 'up', 'wouldn', 'and', 'out', 'below', 'only', 'having', 'through', \"doesn't\", 'doesn', 'just', \n    'where', 'to', 'how', 'then', 'did', 'down', 'each', 'had', \"you'd\", 'herself', 'him', 'myself', 'hadn', 'you',\n    'have', 'ma', \"shan't\", 'if', 'as', 'be', 'doing', 'being', 'an', 'than', 'didn', 'further', 'there', \"isn't\", \n    'themselves', 'any', 'more', \"hasn't\", 'from', \"haven't\", 'not', 've', 'all', 'we', 'again', 'at', 'am', 'such',\n    'with', 'll', 'shouldn', 'of', 'do', 'against', 'both', \"weren't\", 'should', 'this', \"aren't\", 'shan', 'own', \"couldn't\", \n    \"you're\", 'o', 'during', \"don't\", 'what', 'haven', 'does', 'isn', 'which', 'don', 'theirs', 'in', \"it's\", 'for', 'was',\n    'some', \"you've\", 'been', 'they', 'them', 'mustn', 'most', 'here', 'himself', \"wasn't\", 'that', 'their', 'can', \"should've\",\n    \"wouldn't\", 'yourself', 'your', 'ours', 'after', 'on', 'few', 'y', 'off', 'until', 'it', 'her', 'our', 'weren', 'under', 'nor',\n    'his', \"didn't\", \"that'll\", 'ain', 'needn', 'yourselves', 'the', 'because', 'very', 'whom', 'couldn', 'hasn', 'itself', \"needn't\",\n    'yours', 'won', 'these', 'were', 'other', 'between', 't', 'when', 'ourselves', \"shouldn't\", 'its', 'those', \"hadn't\", 'mightn',\n    'wasn', 'm', 'once', \"you'll\", 'the', 'this', 'it', 'its'\n]","metadata":{"execution":{"iopub.status.busy":"2023-08-11T15:44:07.453852Z","iopub.execute_input":"2023-08-11T15:44:07.454401Z","iopub.status.idle":"2023-08-11T15:44:07.474942Z","shell.execute_reply.started":"2023-08-11T15:44:07.454354Z","shell.execute_reply":"2023-08-11T15:44:07.473515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Defining functions that will be applied to the Text columns to clean it and prepare for Vectorization.","metadata":{}},{"cell_type":"code","source":"# Text cleaning\ndef clean_text(text):\n    # Remove special characters and symbols\n    text = text.lower()\n    cleaned_text = re.sub(r'[^a-zA-Z\\s]', '', text)\n    return cleaned_text\n\n\n# Tokenization\ndef tokenize_text(text):\n    tokens = text.split()\n    return tokens\n\n\n# Stop word removal\ndef remove_stopwords(tokens):\n    stopword_list = set(stop_words)\n    filtered_tokens = [token for token in tokens if token not in stopword_list]\n    return filtered_tokens\n","metadata":{"execution":{"iopub.status.busy":"2023-08-11T15:44:07.476454Z","iopub.execute_input":"2023-08-11T15:44:07.476813Z","iopub.status.idle":"2023-08-11T15:44:07.493325Z","shell.execute_reply.started":"2023-08-11T15:44:07.476783Z","shell.execute_reply":"2023-08-11T15:44:07.491960Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_train.columns","metadata":{"execution":{"iopub.status.busy":"2023-08-11T15:44:07.495339Z","iopub.execute_input":"2023-08-11T15:44:07.495849Z","iopub.status.idle":"2023-08-11T15:44:07.509211Z","shell.execute_reply.started":"2023-08-11T15:44:07.495806Z","shell.execute_reply":"2023-08-11T15:44:07.507868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Applying the text cleaning functions to the text columns in the train and test sets.","metadata":{}},{"cell_type":"code","source":"merged_train['reviewText'] = merged_train['reviewText'].apply(clean_text)\nmerged_train['reviewText'] = merged_train['reviewText'].apply(tokenize_text)\nmerged_train['reviewText'] = merged_train['reviewText'].apply(remove_stopwords)\nmerged_train['reviewText'] = merged_train['reviewText'].apply(lambda x: ' '.join(x))\n\nmerged_train['title'] = merged_train['title'].apply(clean_text)\nmerged_train['title'] = merged_train['title'].apply(tokenize_text)\nmerged_train['title'] = merged_train['title'].apply(remove_stopwords)\nmerged_train['title'] = merged_train['title'].apply(lambda x: ' '.join(x))","metadata":{"execution":{"iopub.status.busy":"2023-08-11T15:44:07.511189Z","iopub.execute_input":"2023-08-11T15:44:07.511885Z","iopub.status.idle":"2023-08-11T15:44:15.338050Z","shell.execute_reply.started":"2023-08-11T15:44:07.511845Z","shell.execute_reply":"2023-08-11T15:44:15.336960Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_test['reviewText'] = merged_test['reviewText'].apply(clean_text)\nmerged_test['reviewText'] = merged_test['reviewText'].apply(tokenize_text)\nmerged_test['reviewText'] = merged_test['reviewText'].apply(remove_stopwords)\nmerged_test['reviewText'] = merged_test['reviewText'].apply(lambda x: ' '.join(x))\n\n\nmerged_test['title'] = merged_test['title'].apply(clean_text)\nmerged_test['title'] = merged_test['title'].apply(tokenize_text)\nmerged_test['title'] = merged_test['title'].apply(remove_stopwords)\nmerged_test['title'] = merged_test['title'].apply(lambda x: ' '.join(x))","metadata":{"execution":{"iopub.status.busy":"2023-08-11T15:44:15.339443Z","iopub.execute_input":"2023-08-11T15:44:15.339809Z","iopub.status.idle":"2023-08-11T15:44:17.676540Z","shell.execute_reply.started":"2023-08-11T15:44:15.339777Z","shell.execute_reply":"2023-08-11T15:44:17.675448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_train.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-11T15:44:17.677901Z","iopub.execute_input":"2023-08-11T15:44:17.678243Z","iopub.status.idle":"2023-08-11T15:44:17.710581Z","shell.execute_reply.started":"2023-08-11T15:44:17.678215Z","shell.execute_reply":"2023-08-11T15:44:17.709189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Let's check for the top 30 words after processing the text data and removing stopwords.","metadata":{}},{"cell_type":"code","source":"corpus = merged_train['reviewText']\nvectorizer = CountVectorizer()\nX = vectorizer.fit_transform(corpus)\ncounts = np.sum(X.toarray(), axis=0)\n\nwords = vectorizer.get_feature_names_out()\n\ncount_dict = {}\nfor i in range(len(counts)):\n    count_dict[words[i]] = counts[i]\n\ncount_dict = {k:v for k, v in sorted(count_dict.items(), key=lambda item: item[1], reverse=True)}\n","metadata":{"execution":{"iopub.status.busy":"2023-08-11T15:44:17.712430Z","iopub.execute_input":"2023-08-11T15:44:17.712898Z","iopub.status.idle":"2023-08-11T15:45:13.456719Z","shell.execute_reply.started":"2023-08-11T15:44:17.712853Z","shell.execute_reply":"2023-08-11T15:45:13.455383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,8))\nsns.barplot(x=list(count_dict.keys())[:30],y=list(count_dict.values())[:30])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-11T15:45:13.458783Z","iopub.execute_input":"2023-08-11T15:45:13.459243Z","iopub.status.idle":"2023-08-11T15:45:14.200290Z","shell.execute_reply.started":"2023-08-11T15:45:13.459203Z","shell.execute_reply":"2023-08-11T15:45:14.199458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### As we can see, there are a lot less stop words as compared to the previous barplot. The irrelevant words such as 'the', 'it', 'its' etc. have been removed and now we have the relevant words in our dataset. \n#### However, it must be considered that because we are not using any Natural Language Processing libraries, the level of cleanliness of the text data will be very elss as compared to libraries such as nltk or wordnet since we are using a custom list of stopwords which only has a  limited number of stop words whereas the libraries contain a vast vocabulary for the same purpose.","metadata":{}},{"cell_type":"code","source":"X, y = merged_train.drop('sentiment', axis=1), merged_train['sentiment']","metadata":{"execution":{"iopub.status.busy":"2023-08-11T15:45:14.201547Z","iopub.execute_input":"2023-08-11T15:45:14.202058Z","iopub.status.idle":"2023-08-11T15:45:14.243787Z","shell.execute_reply.started":"2023-08-11T15:45:14.202028Z","shell.execute_reply":"2023-08-11T15:45:14.242631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.drop(columns=\"genre\",inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T15:45:14.246392Z","iopub.execute_input":"2023-08-11T15:45:14.247132Z","iopub.status.idle":"2023-08-11T15:45:14.294857Z","shell.execute_reply.started":"2023-08-11T15:45:14.247070Z","shell.execute_reply":"2023-08-11T15:45:14.293361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Constructing a Column Transformer to apply Encoding and Scaling ","metadata":{}},{"cell_type":"code","source":"transformer = ColumnTransformer([\n    (\"title_tfidf\",TfidfVectorizer(ngram_range=(1,3), min_df=0.00001), \"title\"),\n    (\"review_tfidf\", TfidfVectorizer(ngram_range=(1,3), min_df=0.00001), 'reviewText'),\n    (\"frequentReviewer_encoder\",OrdinalEncoder(),[\"isFrequentReviewer\"]),\n    (\"score_scaler\",MaxAbsScaler(), [\"audienceScore\"])\n], remainder='passthrough', n_jobs=-1, verbose=True)\n# feature_selector = SelectKBest(score_func=chi2, k=500000)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T15:45:14.296900Z","iopub.execute_input":"2023-08-11T15:45:14.297808Z","iopub.status.idle":"2023-08-11T15:45:14.305523Z","shell.execute_reply.started":"2023-08-11T15:45:14.297759Z","shell.execute_reply":"2023-08-11T15:45:14.304067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Let's check how our tranformed data looks like","metadata":{}},{"cell_type":"code","source":"feature_matrix = transformer.fit_transform(X)\nfeature_matrix.shape","metadata":{"execution":{"iopub.status.busy":"2023-08-11T15:45:14.307962Z","iopub.execute_input":"2023-08-11T15:45:14.309440Z","iopub.status.idle":"2023-08-11T15:45:35.899786Z","shell.execute_reply.started":"2023-08-11T15:45:14.309401Z","shell.execute_reply":"2023-08-11T15:45:35.898117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### The transformed feature_matrix looks has a lot of features compared to when it was not transformed that's because most of our features were text columns and the words in those columns have been converted into features and thus resulting in a large number of features. \n#### We will be using Feature selection tehcniques to see which one gives us the best results.","metadata":{}},{"cell_type":"markdown","source":"# Logistic Regression Pipeline","metadata":{}},{"cell_type":"code","source":"logistic_pipe = Pipeline([(\"transformer\", transformer),\n#                           (\"feature_selector\", feature_selector),\n                          ('LogReg', LogisticRegressionCV(random_state=10,max_iter=300, solver='liblinear', penalty='l2',fit_intercept=True,cv=5, n_jobs=-1))],\n               verbose=True)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T17:11:00.073694Z","iopub.execute_input":"2023-08-11T17:11:00.074144Z","iopub.status.idle":"2023-08-11T17:11:00.081625Z","shell.execute_reply.started":"2023-08-11T17:11:00.074110Z","shell.execute_reply":"2023-08-11T17:11:00.080377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# param_dist = {\n#     'LogReg__fit_intercept': [True, False],\n#     'LogReg__class_weight': [None, 'balanced'],\n#     'LogReg__penalty': ['l1', 'l2', 'elasticnet'],\n#     'LogReg__tol': [1e-4, 1e-3, 1e-2]\n    \n# }\n# random_search = RandomizedSearchCV(\n#     logistic_pipe,\n#     param_distributions=param_dist,\n#     n_iter=10,      \n#     n_jobs=-1,  \n#     verbose=2  \n# )\n# random_search.fit(X, y)\n# best_hyperparameters = random_search.best_params_\n# best_model = random_search.best_estimator_\n# print(best_hyperparameters, best_model)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-08-11T17:11:02.146348Z","iopub.execute_input":"2023-08-11T17:11:02.146745Z","iopub.status.idle":"2023-08-11T17:11:02.153432Z","shell.execute_reply.started":"2023-08-11T17:11:02.146715Z","shell.execute_reply":"2023-08-11T17:11:02.151803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logistic_pipe.fit(X,y)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T17:11:02.703526Z","iopub.execute_input":"2023-08-11T17:11:02.703951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_pred_logistic = logistic_pipe.predict(X)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:13:50.809164Z","iopub.execute_input":"2023-08-11T16:13:50.809801Z","iopub.status.idle":"2023-08-11T16:14:08.333600Z","shell.execute_reply.started":"2023-08-11T16:13:50.809738Z","shell.execute_reply":"2023-08-11T16:14:08.331979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_logistic = logistic_pipe.predict(merged_test)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:14:08.335887Z","iopub.execute_input":"2023-08-11T16:14:08.336799Z","iopub.status.idle":"2023-08-11T16:14:15.603932Z","shell.execute_reply.started":"2023-08-11T16:14:08.336755Z","shell.execute_reply":"2023-08-11T16:14:15.602514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logistic_pipe.score(X,y) ","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:14:15.605870Z","iopub.execute_input":"2023-08-11T16:14:15.606329Z","iopub.status.idle":"2023-08-11T16:14:30.369224Z","shell.execute_reply.started":"2023-08-11T16:14:15.606286Z","shell.execute_reply":"2023-08-11T16:14:30.367631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(skm.classification_report(y,train_pred_logistic))","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:14:30.370854Z","iopub.execute_input":"2023-08-11T16:14:30.371264Z","iopub.status.idle":"2023-08-11T16:14:45.215917Z","shell.execute_reply.started":"2023-08-11T16:14:30.371232Z","shell.execute_reply":"2023-08-11T16:14:45.214490Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"conf_mat = skm.confusion_matrix(y,train_pred_logistic)\ndisp = skm.ConfusionMatrixDisplay(confusion_matrix=conf_mat, display_labels=logistic_pipe[-1].classes_)\ndisp.plot()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:14:45.217733Z","iopub.execute_input":"2023-08-11T16:14:45.218242Z","iopub.status.idle":"2023-08-11T16:14:47.175604Z","shell.execute_reply.started":"2023-08-11T16:14:45.218196Z","shell.execute_reply":"2023-08-11T16:14:47.174348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_svc = skm.PrecisionRecallDisplay.from_estimator(\n    logistic_pipe, X, y, name=\"LogisticRegression\"\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:14:47.177534Z","iopub.execute_input":"2023-08-11T16:14:47.178026Z","iopub.status.idle":"2023-08-11T16:15:02.976934Z","shell.execute_reply.started":"2023-08-11T16:14:47.177981Z","shell.execute_reply":"2023-08-11T16:15:02.975721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SVM Pipeline","metadata":{}},{"cell_type":"code","source":"pipe_svc = Pipeline([(\"transformer\", transformer),\n#                      (\"feature_selector\", feature_selector),\n                 ('Svm', LinearSVC(C=0.20584494295802447,class_weight='balanced',dual=False,max_iter=2000,penalty='l2',tol=0.0001))],\n               verbose=True)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:15:02.979036Z","iopub.execute_input":"2023-08-11T16:15:02.979867Z","iopub.status.idle":"2023-08-11T16:15:02.987682Z","shell.execute_reply.started":"2023-08-11T16:15:02.979807Z","shell.execute_reply":"2023-08-11T16:15:02.986417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# param_grid = {\n#     'Svm__C': uniform(loc=0, scale=10),  # Regularization parameter C for LinearSVC\n#     'Svm__max_iter': [1000, 2000, 3000],  # Maximum number of iterations for LinearSVC\n#     'Svm__dual': [True, False],  # Use dual or primal formulation for LinearSVC\n#     'Svm__class_weight': [None, 'balanced'],\n#     'Svm__tol': [1e-3, 1e-4, 1e-5],  # Tolerance for stopping criterion\n#     'Svm__penalty': ['l1', 'l2'],  # Norm used in the penalty\n#     'Svm__fit_intercept': [True, False] # Class weight for LinearSVC\n# }\n# random_search = RandomizedSearchCV(\n#     pipe_svc,  # The pipeline\n#     param_distributions=param_grid,  # The hyperparameter grid\n#     n_iter=10,  # Number of random parameter combinations to try\n#     scoring='accuracy',  # Use accuracy as the evaluation metric\n#     n_jobs=-1,  # Use all available CPU cores for faster computation\n#     cv=5,  # Cross-validation fold\n#     random_state=42  # For reproducibility\n# )\n\n\n# random_search.fit(X,y)\n\n# best_params = random_search.best_params_\n# best_model = random_search.best_estimator_\n\n# print(best_params)\n# print(best_model)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:15:02.989678Z","iopub.execute_input":"2023-08-11T16:15:02.990459Z","iopub.status.idle":"2023-08-11T16:15:03.003414Z","shell.execute_reply.started":"2023-08-11T16:15:02.990411Z","shell.execute_reply":"2023-08-11T16:15:03.001624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipe_svc.fit(X,y)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:15:03.005227Z","iopub.execute_input":"2023-08-11T16:15:03.008874Z","iopub.status.idle":"2023-08-11T16:16:13.611309Z","shell.execute_reply.started":"2023-08-11T16:15:03.008819Z","shell.execute_reply":"2023-08-11T16:16:13.609999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_pred_svc = pipe_svc.predict(X)\ny_pred_svc = pipe_svc.predict(merged_test)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:16:13.613281Z","iopub.execute_input":"2023-08-11T16:16:13.613665Z","iopub.status.idle":"2023-08-11T16:16:35.215781Z","shell.execute_reply.started":"2023-08-11T16:16:13.613631Z","shell.execute_reply":"2023-08-11T16:16:35.214737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipe_svc.score(X,y)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:16:35.217147Z","iopub.execute_input":"2023-08-11T16:16:35.217955Z","iopub.status.idle":"2023-08-11T16:16:50.218954Z","shell.execute_reply.started":"2023-08-11T16:16:35.217921Z","shell.execute_reply":"2023-08-11T16:16:50.217505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(skm.classification_report(y,train_pred_svc))","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:16:50.220247Z","iopub.execute_input":"2023-08-11T16:16:50.220607Z","iopub.status.idle":"2023-08-11T16:17:05.264640Z","shell.execute_reply.started":"2023-08-11T16:16:50.220578Z","shell.execute_reply":"2023-08-11T16:17:05.263341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"conf_mat = skm.confusion_matrix(y,train_pred_svc)\ndisp = skm.ConfusionMatrixDisplay(confusion_matrix=conf_mat, display_labels=pipe_svc[-1].classes_)\ndisp.plot()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:17:05.266401Z","iopub.execute_input":"2023-08-11T16:17:05.267699Z","iopub.status.idle":"2023-08-11T16:17:07.560263Z","shell.execute_reply.started":"2023-08-11T16:17:05.267651Z","shell.execute_reply":"2023-08-11T16:17:07.559277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_svc = skm.PrecisionRecallDisplay.from_estimator(\n    pipe_svc, X, y, name=\"LinearSVC\"\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:17:07.561804Z","iopub.execute_input":"2023-08-11T16:17:07.562504Z","iopub.status.idle":"2023-08-11T16:17:23.833628Z","shell.execute_reply.started":"2023-08-11T16:17:07.562426Z","shell.execute_reply":"2023-08-11T16:17:23.832356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Bagging Pipeline","metadata":{}},{"cell_type":"code","source":"pipe_bagging = Pipeline([(\"transformer\", transformer),\n#                         (\"feature_selector\", feature_selector),\n                        ('Bagging', BaggingClassifier(estimator=LogisticRegression()))],\n               verbose=True)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:41:09.938793Z","iopub.execute_input":"2023-08-11T16:41:09.939446Z","iopub.status.idle":"2023-08-11T16:41:09.947782Z","shell.execute_reply.started":"2023-08-11T16:41:09.939393Z","shell.execute_reply":"2023-08-11T16:41:09.945972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# param_dist = {\n#     'Bagging__n_estimators': [10, 50, 100], \n#     'Bagging__max_samples': [0.5, 0.7, 0.9],  \n#     'Bagging__max_features': [0.5, 0.7, 0.9],  \n#     'Bagging__bootstrap': [True, False],  \n#     'Bagging__bootstrap_features': [True, False]\n# }","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:17:24.297614Z","iopub.status.idle":"2023-08-11T16:17:24.298252Z","shell.execute_reply.started":"2023-08-11T16:17:24.297899Z","shell.execute_reply":"2023-08-11T16:17:24.297920Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Create the RandomizedSearchCV object\n# random_search = RandomizedSearchCV(\n#     estimator=pipe_bagging,\n#     param_distributions=param_dist,\n#     n_iter=10, \n#     scoring='f1_micro',  \n#     cv=5,  \n#     verbose=2,  \n#     n_jobs=-1,  \n#     random_state=42  \n# )\n\n# random_search.fit(X, y)\n\n# print(\"Best Parameters: \", random_search.best_params_)\n# print(\"Best F1 Score: \", random_search.best_score_)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:17:24.300555Z","iopub.status.idle":"2023-08-11T16:17:24.301037Z","shell.execute_reply.started":"2023-08-11T16:17:24.300821Z","shell.execute_reply":"2023-08-11T16:17:24.300844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipe_bagging.fit(X,y)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:41:16.920530Z","iopub.execute_input":"2023-08-11T16:41:16.921023Z","iopub.status.idle":"2023-08-11T16:45:44.655900Z","shell.execute_reply.started":"2023-08-11T16:41:16.920974Z","shell.execute_reply":"2023-08-11T16:45:44.654387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_pred_bagging = pipe_bagging.predict(X)\ny_pred_bagging = pipe_bagging.predict(merged_test)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:45:49.497411Z","iopub.execute_input":"2023-08-11T16:45:49.497831Z","iopub.status.idle":"2023-08-11T16:46:13.799443Z","shell.execute_reply.started":"2023-08-11T16:45:49.497799Z","shell.execute_reply":"2023-08-11T16:46:13.798173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipe_bagging.score(X,y)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:46:13.801875Z","iopub.execute_input":"2023-08-11T16:46:13.802322Z","iopub.status.idle":"2023-08-11T16:46:30.884327Z","shell.execute_reply.started":"2023-08-11T16:46:13.802286Z","shell.execute_reply":"2023-08-11T16:46:30.882964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(skm.classification_report(y,train_pred_bagging))","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:46:30.886297Z","iopub.execute_input":"2023-08-11T16:46:30.886801Z","iopub.status.idle":"2023-08-11T16:46:45.981178Z","shell.execute_reply.started":"2023-08-11T16:46:30.886766Z","shell.execute_reply":"2023-08-11T16:46:45.979821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"conf_mat = skm.confusion_matrix(y,train_pred_bagging)\ndisp = skm.ConfusionMatrixDisplay(confusion_matrix=conf_mat, display_labels=pipe_bagging[-1].classes_)\ndisp.plot()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:46:45.984602Z","iopub.execute_input":"2023-08-11T16:46:45.986186Z","iopub.status.idle":"2023-08-11T16:46:48.015585Z","shell.execute_reply.started":"2023-08-11T16:46:45.986128Z","shell.execute_reply":"2023-08-11T16:46:48.014271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_bagging= skm.PrecisionRecallDisplay.from_estimator(\n    pipe_bagging, X, y, name=\"BaggingClassifier\"\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:46:48.017551Z","iopub.execute_input":"2023-08-11T16:46:48.018322Z","iopub.status.idle":"2023-08-11T16:47:05.687351Z","shell.execute_reply.started":"2023-08-11T16:46:48.018277Z","shell.execute_reply":"2023-08-11T16:47:05.685935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Multi-Layer Perceptron Pipeline","metadata":{}},{"cell_type":"code","source":"# pipe_perceptron = Pipeline([(\"transformer\", transformer),\n#                  ('MLP', MLPClassifier(max_iter=20,random_state=1,hidden_layer_sizes=(20,20, 50)))],\n#                verbose=True)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:47:05.689259Z","iopub.execute_input":"2023-08-11T16:47:05.689779Z","iopub.status.idle":"2023-08-11T16:47:05.696076Z","shell.execute_reply.started":"2023-08-11T16:47:05.689733Z","shell.execute_reply":"2023-08-11T16:47:05.694658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pipe_perceptron.fit(X,y)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:47:05.698006Z","iopub.execute_input":"2023-08-11T16:47:05.698821Z","iopub.status.idle":"2023-08-11T16:47:05.709755Z","shell.execute_reply.started":"2023-08-11T16:47:05.698774Z","shell.execute_reply":"2023-08-11T16:47:05.708468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_pred_perceptron = pipe_perceptron.predict(X)\n# y_pred_perceptron = pipe_perceptron.predict(merged_test)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:47:05.711640Z","iopub.execute_input":"2023-08-11T16:47:05.712502Z","iopub.status.idle":"2023-08-11T16:47:05.723269Z","shell.execute_reply.started":"2023-08-11T16:47:05.712454Z","shell.execute_reply":"2023-08-11T16:47:05.722025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pipe_perceptron.score(X,y)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:47:05.725242Z","iopub.execute_input":"2023-08-11T16:47:05.725730Z","iopub.status.idle":"2023-08-11T16:47:05.734989Z","shell.execute_reply.started":"2023-08-11T16:47:05.725697Z","shell.execute_reply":"2023-08-11T16:47:05.734100Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating Submission file","metadata":{}},{"cell_type":"code","source":"submission_df = pd.DataFrame({'sentiment': y_pred_logistic})\nsubmission_df.index.names = ['id']\nsubmission_df.to_csv('submission.csv')","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:47:05.738178Z","iopub.execute_input":"2023-08-11T16:47:05.738524Z","iopub.status.idle":"2023-08-11T16:47:05.921667Z","shell.execute_reply.started":"2023-08-11T16:47:05.738494Z","shell.execute_reply":"2023-08-11T16:47:05.920353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.shape","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:47:05.923035Z","iopub.execute_input":"2023-08-11T16:47:05.923411Z","iopub.status.idle":"2023-08-11T16:47:05.931769Z","shell.execute_reply.started":"2023-08-11T16:47:05.923380Z","shell.execute_reply":"2023-08-11T16:47:05.930107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:47:05.934363Z","iopub.execute_input":"2023-08-11T16:47:05.935203Z","iopub.status.idle":"2023-08-11T16:47:05.951431Z","shell.execute_reply.started":"2023-08-11T16:47:05.935141Z","shell.execute_reply":"2023-08-11T16:47:05.949831Z"},"trusted":true},"execution_count":null,"outputs":[]}]}